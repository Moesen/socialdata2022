{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Exercise 2**\n",
    "## **Exercise 2.1**\n",
    "- What are the most common types of data bias\n",
    "    - **Response bias**: Occurs in content generated by humans such as reviews on Trustpilot, Wikipedia entries, etc. Happens because the data comes from a certain specific set of sources, which are a small subset of the whole population. Amazon reviews is an example, where 4% of the total users, are responsible for 50% of all reviews on amazon.\n",
    "    - **Bias due to feedback loop**: Occurs when a model itself influences the generation of data that is used to train it. ML models direct user attention to a small subset of items. An example of this is netflix *(ntflx)*, where the user is provided a list of recommended movies, and based on this, Netflix might have decided to use the consumption behavior to further improve their recommendation system. More formally; this type of bias occurs due to the non-random subset of items presented to users.\n",
    "    - **Bias due to system drift**: Occurs when the system generating the data goes through changes over time. Two types of drift: 1) Concept drift, where the definition of the target changes, 2) Model drift, where the way the users interact changes.\n",
    "    - **Omitted variable bias**: Occurs in data in which critical attributes that influence the outcome are missing. This generally occurs when the data is generated by humans, not recording certain info due to lack of access and/or privacy.\n",
    "    - **Societal Bias**: Occurs in content produced by humans, whether it be social media content or curated news articles. Data inherently contains bias and stereotypes. Models trained on data will discriminate on race, gender or other categories. This is seen in the article (*propublica*), where the machine learning model in the shown cases discriminates against the race of the person.\n",
    "\n",
    "<br>\n",
    "- What are the potential bias sources/types in our case-study\n",
    "\n",
    "## **Exercise 2.2** - Equal odds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References:\n",
    "[Netflix Machine larning *(ntflx)*](https://research.netflix.com/research-area/machine-learning)\n",
    "\n",
    "[Propublica - Machine bias risk-assesments in criminal sentencing (*propublica*)](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "baa125a6947e7fe2d780fda829e288a844d09a2a427c95ce6f67ebd33720e2ef"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('sem10')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
